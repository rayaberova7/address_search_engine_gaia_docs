@article{zheng2022change,
    author = {Zhuo Zheng and others},
    title = {Change Is Everywhere: Single-Temporal Supervised Object Change Detection in Remote Sensing Imagery},
    journal = {arXiv},
    month = {August},
    year = {2022},
    eprint = {2108.07002},
    url = {http://arxiv.org/abs/2108.07002},
    note = {Accessed on April 4, 2023},
}

@techreport{berova2023,
    author = {Raya Berova},
    title = {{Détection automatisée de changements des bâtiments en France d'Outre-Mer à partir d'images satellites}},
    institution = {Direction Régionale de l'Insee Martinique, Direction Générale de l'Insee},
    year = {2023},
    type = {Rapport de stage},
    url = {https://minio.lab.sspcloud.fr/cguillo/rapport_stage/Rapport_de_stage_3A-2.pdf}
}

@article{bendjoudi_coefficient_2002,
	title = {Le coefficient de compacité de {Gravelius}: analyse critique d'un indice de forme des bassins versants},
	volume = {47},
	issn = {0262-6667, 2150-3435},
	shorttitle = {Le coefficient de compacité de {Gravelius}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02626660209493000},
	doi = {10.1080/02626660209493000},
	language = {fr},
	number = {6},
	urldate = {2024-05-07},
	journal = {Hydrological Sciences Journal},
	author = {Bendjoudi, Hocine and Hubert, Pierre},
	month = dec,
	year = {2002},
	pages = {921--930},
	file = {Bendjoudi et Hubert - 2002 - Le coefficient de compacité de Gravelius analyse .pdf:C\:\\Users\\RK09OA\\Zotero\\storage\\7SRJ23RR\\Bendjoudi et Hubert - 2002 - Le coefficient de compacité de Gravelius analyse .pdf:application/pdf},
}

@techreport{nabec2023,
    author = {Judith Nabec},
    title = {{Mise en place d'une méthode de détection automatique des évolutions de bâti en Outre-Mer sur des images satellites}},
    institution = {Direction Générale de l'Insee},
    year = {2023},
    type = {Rapport de stage},
    url = {https://minio.lab.sspcloud.fr/cguillo/rapport_stage/Rapport_de_stage_2023-3.pdf}
}

@techreport{chabennet2021,
    author = {Quentin Chabennet},
    title = {{Détection automatique de la couverture des sols à partir d’images satellites à l’aide de méthodes de segmentation sémantique}},
    institution = {ECE engineering school},
    year = {2021},
    type = {Mémoire de stage},
    url = {https://minio.lab.sspcloud.fr/cguillo/rapport_stage/rapport_stage_quentin_dmrg-1.pdf}
}


@article{xie2021segformer,
  title={SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers},
  author={Xie, Enze and Wang, Wenhai and Yuille, Alan L. and Anandkumar, Anima and Alvarez, Jose M.},
  journal={arXiv preprint arXiv:2105.15203},
  year={2021},
  month={October},
  url={http://arxiv.org/abs/2105.15203}
}

@article{chen2017rethinking,
  title={Rethinking Atrous Convolution for Semantic Image Segmentation},
  author={Chen, Liang-Chieh and et al.},
  journal={arXiv},
  month={December},
  year={2017},
  eprint={1706.05587},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  accessed={April 25, 2023},
  url={http://arxiv.org/abs/1706.05587}
}



@inproceedings{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017},
  organization={Curran Associates, Inc.},
  url={https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}
@article{dosovitskiy2021image,
  title={An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2021},
  month={June},
  url={http://arxiv.org/abs/2010.11929}
}


 % à utiliser pour parler de la segmentation sémantique




  @inproceedings{Albawi_Mohammed_Al-Zawi_2017, title={Understanding of a convolutional neural network}, DOI={10.1109/ICEngTechnol.2017.8308186}, abstractNote={The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers. Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fully-connected layer. The convolutional and fully-connected layers have parameters but pooling and non-linearity layers don’t have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing. In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.}, booktitle={2017 International Conference on Engineering and Technology (ICET)}, author={Albawi, Saad and Mohammed, Tareq Abed and Al-Zawi, Saad}, year={2017}, month={Aug}, pages={1–6} }
 
 @inproceedings{Constantin_Ding_Lee_2018, title={Accurate Road Detection from Satellite Images Using Modified U-net}, DOI={10.1109/APCCAS.2018.8605652}, abstractNote={In this paper, we present an accurate neural network algorithm to detect roads in satellite images. Based on convolutional neural networks, from a 6-channel image, this model is able to transfer the road structure to the output using both the U-net and the atrous convolution architecture. To train this model, we introduce a new combination of existing loss functions including the binary cross-entropy and the Jaccard distance to avoid false positive detection and increase binary classification accuracy. In terms of precision, recall, the F-score and accuracy, experiments carried out using the Massachusetts roads dataset, provide better results than state-of-the-art road extraction models.}, booktitle={2018 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)}, author={Constantin, Alexandre and Ding, Jian-Jiun and Lee, Yih-Cherng}, year={2018}, month={Oct}, pages={423–426} }
 
 @article{Garcia-Garcia_Orts-Escolano_Oprea_Villena-Martinez_Garcia-Rodriguez_2017, title={A Review on Deep Learning Techniques Applied to Semantic Segmentation}, url={http://arxiv.org/abs/1704.06857}, abstractNote={Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.}, note={arXiv: 1704.06857}, journal={arXiv:1704.06857 [cs]}, author={Garcia-Garcia, Alberto and Orts-Escolano, Sergio and Oprea, Sergiu and Villena-Martinez, Victor and Garcia-Rodriguez, Jose}, year={2017}, month={Apr} }
 
 @article{Isola_Zhu_Zhou_Efros_2018, title={Image-to-Image Translation with Conditional Adversarial Networks}, url={http://arxiv.org/abs/1611.07004}, abstractNote={We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.}, note={arXiv: 1611.07004}, journal={arXiv:1611.07004 [cs]}, author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.}, year={2018}, month={Nov} }
 
 @inproceedings{Ivanovsky_Khryashchev_Pavlov_Ostrovskaya_2019, title={Building Detection on Aerial Images Using U-NET Neural Networks}, ISSN={2305-7254}, DOI={10.23919/FRUCT.2019.8711930}, abstractNote={This article presents research results of two convolutional neural networks for building detection on satellite images of Planet database. To analyze the quality of developed algorithms, there was used Sorensen-Dice coefficient of similarity which compares results of algorithms with tagged masks. The masks were generated from json files and sliced on smaller parts together with respective images before the training of algorithms. This approach allows to cope with the problem of segmentation for aerial high-resolution images efficiently and effectively. The problem of building detection on satellite images can be put into practice for urban planning, building control, etc.}, booktitle={2019 24th Conference of Open Innovations Association (FRUCT)}, author={Ivanovsky, Leonid and Khryashchev, Vladimir and Pavlov, Vladimir and Ostrovskaya, Anna}, year={2019}, month={Apr}, pages={116–122} }
 
 @article{Jégou_Drozdzal_Vazquez_Romero_Bengio_2017, title={The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation}, url={http://arxiv.org/abs/1611.09326}, abstractNote={State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is available here : https://github.com/SimJeg/FC-DenseNet/blob/master/train.py}, note={arXiv: 1611.09326}, journal={arXiv:1611.09326 [cs]}, author={Jégou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua}, year={2017}, month={Oct} }
 
 @inbook{Kim_2017, place={Berkeley, CA}, title={Convolutional Neural Network}, ISBN={978-1-4842-2845-6}, url={https://doi.org/10.1007/978-1-4842-2845-6_6}, DOI={10.1007/978-1-4842-2845-6_6}, abstractNote={The importance of the deep neural network lies in the fact that it opened the door to the complicated non-linear model and systematic approach for the hierarchical processing of knowledge.}, booktitle={MATLAB Deep Learning: With Machine Learning, Neural Networks and Artificial Intelligence}, publisher={Apress}, author={Kim, Phil}, editor={Kim, Phil}, year={2017}, pages={121–147} }
 @article{Long_Shelhamer_Darrell_2015, title={Fully Convolutional Networks for Semantic Segmentation}, url={http://arxiv.org/abs/1411.4038}, abstractNote={Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.}, note={arXiv: 1411.4038}, journal={arXiv:1411.4038 [cs]}, author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor}, year={2015}, month={Mar} }

 @article{Ronneberger_Fischer_Brox_2015b, title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, url={http://arxiv.org/abs/1505.04597}, abstractNote={There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .}, note={arXiv: 1505.04597}, journal={arXiv:1505.04597 [cs]}, author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas}, year={2015}, month={May} }
 
 @article{Sharifi_2021, title={Yield prediction with machine learning algorithms and satellite images}, volume={101}, ISSN={1097-0010}, DOI={10.1002/jsfa.10696}, abstractNote={BACKGROUND Barley is one of the strategic agricultural products available in the world, and yield prediction is important for ensuring food security. One way of estimating a product is to use remote sensing data in conjunction with field data and meteorological data. One of the main issues surrounding this comprises the use of machine learning techniques to create a multi-resource data-based estimation model. Many studies have been conducted on barley yield prediction from planting to harvest. Still, the effect of different time intervals on yield prediction has not been investigated. Furthermore, the effect of different periods on yield prediction has not been investigated. RESULTS In the present study, the whole growth period was divided into three parts. Using one of the major barley production areas in Iran, the performance of the proposed model was evaluated. In the first step, a model for integrating field data, remote sensing data and meteorological data was prepared. The results obtained show that, among the four machine learning methods implemented, the gaussian process regression algorithm performed best and estimated yield with r2 = 0.84, root mean square error = 737 kg ha−1 and mean absolute = 650 kg ha−1, 1 month before harvest. CONCLUSION It was found that the estimation results change depending on different agricultural zones and temporal training settings. The findings of the present study provide a powerful potential tool for the yield prediction of barley using multi-source data and machine learning. © 2020 Society of Chemical Industry}, number={3}, journal={Journal of the Science of Food and Agriculture}, author={Sharifi, Alireza}, year={2021}, pages={891–896} }
 
 @article{Visin_Ciccone_Romero_Kastner_Cho_Bengio_Matteucci_Courville_2016, title={ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation}, url={http://arxiv.org/abs/1511.07053}, abstractNote={We propose a structured prediction architecture, which exploits the local generic features extracted by Convolutional Neural Networks and the capacity of Recurrent Neural Networks (RNN) to retrieve distant dependencies. The proposed architecture, called ReSeg, is based on the recently introduced ReNet model for image classification. We modify and extend it to perform the more challenging task of semantic segmentation. Each ReNet layer is composed of four RNN that sweep the image horizontally and vertically in both directions, encoding patches or activations, and providing relevant global information. Moreover, ReNet layers are stacked on top of pre-trained convolutional layers, benefiting from generic local features. Upsampling layers follow ReNet layers to recover the original image resolution in the final predictions. The proposed ReSeg architecture is efficient, flexible and suitable for a variety of semantic segmentation tasks. We evaluate ReSeg on several widely-used semantic segmentation datasets: Weizmann Horse, Oxford Flower, and CamVid; achieving state-of-the-art performance. Results show that ReSeg can act as a suitable architecture for semantic segmentation tasks, and may have further applications in other structured prediction problems. The source code and model hyperparameters are available on https://github.com/fvisin/reseg.}, note={arXiv: 1511.07053}, journal={arXiv:1511.07053 [cs]}, author={Visin, Francesco and Ciccone, Marco and Romero, Adriana and Kastner, Kyle and Cho, Kyunghyun and Bengio, Yoshua and Matteucci, Matteo and Courville, Aaron}, year={2016}, month={May} }
 
 @article{Wagner_Dalagnol_Tarabalka_Segantine_Thomé_Hirye_2020, title={U-Net-Id, an Instance Segmentation Model for Building Extraction from Satellite Images—Case Study in the Joanópolis City, Brazil}, volume={12}, DOI={10.3390/rs12101544}, abstractNote={Currently, there exists a growing demand for individual building mapping in regions of rapid urban growth in less-developed countries. Most existing methods can segment buildings but cannot discriminate adjacent buildings. Here, we present a new convolutional neural network architecture (CNN) called U-net-id that performs building instance segmentation. The proposed network is trained with WorldView-3 satellite RGB images (0.3 m) and three different labeled masks. The first is the building mask; the second is the border mask, which is the border of the building segment with 4 pixels added outside and 3 pixels inside; and the third is the inner segment mask, which is the segment of the building diminished by 2 pixels. The architecture consists of three parallel paths, one for each mask, all starting with a U-net model. To accurately capture the overlap between the masks, all activation layers of the U-nets are copied and concatenated on each path and sent to two additional convolutional layers before the output activation layers. The method was tested with a dataset of 7563 manually delineated individual buildings of the city of Joanópolis-SP, Brazil. On this dataset, the semantic segmentation showed an overall accuracy of 97.67\% and an F1-Score of 0.937 and the building individual instance segmentation showed good performance with a mean intersection over union (IoU) of 0.582 (median IoU = 0.694).}, number={1010}, journal={Remote Sensing}, publisher={Multidisciplinary Digital Publishing Institute}, author={Wagner, Fabien H. and Dalagnol, Ricardo and Tarabalka, Yuliya and Segantine, Tassiana Y. F. and Thomé, Rogério and Hirye, Mayumi C. M.}, year={2020}, month={Jan}, pages={1544} }

 @inproceedings{Wang_Chen_Yuan_Liu_Huang_Hou_Cottrell_2018, title={Understanding Convolution for Semantic Segmentation}, DOI={10.1109/WACV.2018.00163}, abstractNote={Recent advances in deep learning, especially deep convolutional neural networks (CNNs), have led to significant improvement over previous semantic segmentation systems. Here we show how to improve pixel-wise semantic segmentation by manipulating convolution-related operations that are of both theoretical and practical value. First, we design dense upsampling convolution (DUC) to generate pixel-level prediction, which is able to capture and decode more detailed information that is generally missing in bilinear upsampling. Second, we propose a hybrid dilated convolution (HDC) framework in the encoding phase. This framework 1) effectively enlarges the receptive fields (RF) of the network to aggregate global information; 2) alleviates what we call the “gridding issue”caused by the standard dilated convolution operation. We evaluate our approaches thoroughly on the Cityscapes dataset, and achieve a state-of-art result of 80.1\% mIOU in the test set at the time of submission. We also have achieved state-of-theart overall on the KITTI road estimation benchmark and the PASCAL VOC2012 segmentation task. Our source code can be found at https://github.com/TuSimple/TuSimple-DUC.}, booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)}, author={Wang, Panqu and Chen, Pengfei and Yuan, Ye and Liu, Ding and Huang, Zehua and Hou, Xiaodi and Cottrell, Garrison}, year={2018}, month={Mar}, pages={1451–1460} }
 
 @article{Wieland_Pittore_2014, title={Performance Evaluation of Machine Learning Algorithms for Urban Pattern Recognition from Multi-spectral Satellite Images}, volume={6}, DOI={10.3390/rs6042912}, abstractNote={In this study, a classification and performance evaluation framework for the recognition of urban patterns in medium (Landsat ETM, TM and MSS) and very high resolution (WorldView-2, Quickbird, Ikonos) multi-spectral satellite images is presented. The study aims at exploring the potential of machine learning algorithms in the context of an object-based image analysis and to thoroughly test the algorithm’s performance under varying conditions to optimize their usage for urban pattern recognition tasks. Four classification algorithms, Normal Bayes, K Nearest Neighbors, Random Trees and Support Vector Machines, which represent different concepts in machine learning (probabilistic, nearest neighbor, tree-based, function-based), have been selected and implemented on a free and open-source basis. Particular focus is given to assess the generalization ability of machine learning algorithms and the transferability of trained learning machines between different image types and image scenes. Moreover, the influence of the number and choice of training data, the influence of the size and composition of the feature vector and the effect of image segmentation on the classification accuracy is evaluated.}, number={44}, journal={Remote Sensing}, publisher={Multidisciplinary Digital Publishing Institute}, author={Wieland, Marc and Pittore, Massimiliano}, year={2014}, month={Apr}, pages={2912–2939} }


@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for ﬁrst-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efﬁcient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the inﬁnity norm.},
	language = {en},
	urldate = {2021-10-19},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {Kingma et Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:C\:\\Users\\RK09OA\\Zotero\\storage\\TSWYZJEA\\Kingma et Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf},
}

@Article{rs12213523,
AUTHOR = {Malinowski, Radek and Lewiński, Stanisław and Rybicki, Marcin and Gromny, Ewa and Jenerowicz, Małgorzata and Krupiński, Michał and Nowakowski, Artur and Wojtkowski, Cezary and Krupiński, Marcin and Krätzschmar, Elke and Schauer, Peter},
TITLE = {Automated Production of a Land Cover/Use Map of Europe Based on Sentinel-2 Imagery},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {21},
ARTICLE-NUMBER = {3523},
URL = {https://www.mdpi.com/2072-4292/12/21/3523},
ISSN = {2072-4292},
ABSTRACT = {Up-to-date information about the Earth&rsquo;s surface provided by land cover maps is essential for numerous environmental and land management applications. There is, therefore, a clear need for the continuous and reliable monitoring of land cover and land cover changes. The growing availability of high resolution, regularly collected remote sensing data can support the increasing number of applications that require high spatial resolution products that are frequently updated (e.g., annually). However, large-scale operational mapping requires a highly-automated data processing workflow, which is currently lacking. To address this issue, we developed a methodology for the automated classification of multi-temporal Sentinel-2 imagery. The method uses a random forest classifier and existing land cover/use databases as the source of training samples. In order to demonstrate its operability, the method was implemented on a large part of the European continent, with CORINE Land Cover and High-Resolution Layers as training datasets. A land cover/use map for the year 2017 was produced, composed of 13 classes. An accuracy assessment, based on nearly 52,000 samples, revealed high thematic overall accuracy (86.1\%) on a continental scale, and average overall accuracy of 86.5\% at country level. Only low-frequency classes obtained lower accuracies and we recommend that their mapping should be improved in the future. Additional modifications to the classification legend, notably the fusion of thematically and spectrally similar vegetation classes, increased overall accuracy to 89.0\%, and resulted in ten, general classes. A crucial aspect of the presented approach is that it embraces all of the most important elements of Earth observation data processing, enabling accurate and detailed (10 m spatial resolution) mapping with no manual user involvement. The presented methodology demonstrates possibility for frequent and repetitive operational production of large-scale land cover maps.},
DOI = {10.3390/rs12213523}
}



